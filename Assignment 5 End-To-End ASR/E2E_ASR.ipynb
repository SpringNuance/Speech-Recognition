{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **End-To-End Automatic Speech Recognition**\n",
    "\n",
    "The E2E models typically consist of one model that does all the work, in comparison to the conventional models that are composed of acoustic model, language model and lexicon.\n",
    "There are two main approaches for doing E2E ASR, the connectionist temporal classification (CTC) and attention-based encoder-decoder (AED).\n",
    "\n",
    "In this exercise, we will see a simple example of doing E2E ASR using attention-based encoder-decoder architecture. The model is pre-trained on the LibriSpeech dataset, which contains about 1000 hours of recordings. \n",
    "\n",
    "Throughout the exercise, you will test the performance of the model on various test sets and see how the performance varies depending on the test set being used. Additionally, you will familiarize yourself with how the decoding is done and what are some advantages and drawbacks of it.\n",
    "\n",
    "The answers should be uploaded to MyCourses page in a PDF file that contains only the questions and the answers. Notebook exports are not accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data preparation**\n",
    "\n",
    "The first thing that we need to do it to prepare the data . We will start by importing the necessary libraries. The model is developed using the Pytorch deep learning framework. More details about Pytorch can be found [here](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in c:\\users\\springnuance\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in c:\\users\\springnuance\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jiwer) (8.1.3)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in c:\\users\\springnuance\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jiwer) (3.5.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\springnuance\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click<9.0.0,>=8.1.3->jiwer) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# The 'numpy' library contains functions for various vector and matrix operations\n",
    "import numpy as np\n",
    "\n",
    "# jiwer is a library for calculating the WER\n",
    "!pip install jiwer\n",
    "from jiwer import wer\n",
    "\n",
    "# 'torch' is the deep learning framework that we are going to use to develop, train and test the model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "import torch.optim as optim\n",
    "\n",
    "# 'prepare_data' contain various functions for preparing the data for training and inference\n",
    "import prepare_data\n",
    "# 'train' contains the code used for training the model\n",
    "from train import train\n",
    "# 'calculate_wer' contains a script for calculating the word error rate (WER)\n",
    "from calculate_wer import get_word_error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that we get the same results every time we run the exercise, we can set a seed for generating random numbers, using the command `torch.manual_seed(0)`.\n",
    "\n",
    "The Pytorch framework allows the computations to be done on a CPU or a GPU. The command `torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")` checks if a GPU with CUDA is installed, and if it is, it will run the computations on it, otherwise it will run everything on the CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below loads the test data that we will use to evaluate the model. For features, we will use log filter banks, with 40 filters. The targets consist of manually annotated transcripts.\n",
    "\n",
    "1. The `test_clean` is a subset of the LibriSpeech dataset, that has utterances on which the system achieves lower WER.\n",
    "\n",
    "2. The `test_other` is a subset of the LibriSpeech dataset, that has utterances on which the system achieves higher WER.\n",
    "\n",
    "3. The `test_long` is a subser of the LibriSpeech dataset, that has long utterances.\n",
    "\n",
    "You can find more information about the dataset in the [original paper](http://www.danielpovey.com/files/2015_icassp_librispeech.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_clean = prepare_data.load_features('data/test_clean.npy')\n",
    "target_test_clean = prepare_data.load_transcripts('data/test_clean.txt')\n",
    "\n",
    "features_test_other = prepare_data.load_features('data/test_other.npy')\n",
    "target_test_other = prepare_data.load_transcripts('data/test_other.txt')\n",
    "\n",
    "features_test_long = prepare_data.load_features('data/test_long.npy')\n",
    "target_test_long = prepare_data.load_transcripts('data/test_long.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cell below, we can inspect the shape of the features of the first data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1042, 40])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test_clean[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the first value (1042) is the number of frames and the second value (40) is the number of filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be using a pre-trained model, we don't need train and development sets. In case some students want to see how the training runs, we will assign the training and development sets to be same as the test set. That way, there will be some training data to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = features_test_clean\n",
    "target_train = target_test_clean\n",
    "features_dev = features_test_clean\n",
    "target_dev = target_test_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to create dictionaries that map each character to an index and vice versa. The whole character set consists of all the  lower-case characters, plus empty space ` `, ` ' `, and the special tokens `<sos>` and `<eos>`, indicating the start and the end of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<sos>': 1, '<eos>': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, \"'\": 29, ' ': 30}\n"
     ]
    }
   ],
   "source": [
    "char2idx, idx2char = prepare_data.encode_data()\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cell below, we can replace each character in the transcripts with the appropriate index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels to indices\n",
    "indexed_target_train = prepare_data.label_to_idx(target_train, char2idx)\n",
    "indexed_target_dev = prepare_data.label_to_idx(target_dev, char2idx)\n",
    "indexed_target_test_clean = prepare_data.label_to_idx(target_test_clean, char2idx)\n",
    "indexed_target_test_other = prepare_data.label_to_idx(target_test_other, char2idx)\n",
    "indexed_target_test_long = prepare_data.label_to_idx(target_test_long, char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will group the features and the indexed targets in a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine features and labels in a tuple\n",
    "train_data = prepare_data.combine_data(features_train, indexed_target_train)\n",
    "dev_data = prepare_data.combine_data(features_dev, indexed_target_dev)\n",
    "test_clean_data = prepare_data.combine_data(features_test_clean, indexed_target_test_clean)\n",
    "test_other_data = prepare_data.combine_data(features_test_other, indexed_target_test_other)\n",
    "test_long_data = prepare_data.combine_data(features_test_long, indexed_target_test_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell divides the data in equal batches that are used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "pairs_batch_train = DataLoader(dataset=train_data,\n",
    "                    batch_size=batch_size,\n",
    "                    drop_last=True,\n",
    "                    shuffle=False,\n",
    "                    collate_fn=prepare_data.collate,\n",
    "                    pin_memory=True)\n",
    "\n",
    "pairs_batch_dev = DataLoader(dataset=dev_data,\n",
    "                    batch_size=batch_size,\n",
    "                    drop_last=True,\n",
    "                    shuffle=False,\n",
    "                    collate_fn=prepare_data.collate,\n",
    "                    pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building the model**\n",
    "\n",
    "For building the model, we will use attention-based encoder-decoder architecture. For more details about the encoder-decoder architecture, refer to [this](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) Pytorch tutorial.\n",
    "\n",
    "In the figure below, you can see an illustration of the attention-based encoder-decoder architecture. The figure is borrowed from this [blog post](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3).\n",
    "\n",
    "<img src=\"AED.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The architecture consist of encoder and attention decoder. The encoder is a BLSTM that takes the audio features as input and outputs a vector representation of those features. The encoder is defined in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_tensor, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.input_tensor = input_tensor\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # The dropout randomly disconnects neurons during training. It is used to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.1)        \n",
    "\n",
    "        # The definition of the BLSTM cell, which takes the audio features, processes them and returns a vector representation of them\n",
    "        self.lstm = nn.LSTM(\n",
    "                            self.input_tensor,\n",
    "                            self.hidden_size,\n",
    "                            num_layers=self.num_layers,\n",
    "                            bidirectional=True\n",
    "                            )\n",
    "\n",
    "    def forward(self, input_tensor, input_feature_lengths):\n",
    "        input_tensor = pack_padded_sequence(input_tensor, input_feature_lengths)\n",
    "        output, hidden = self.lstm(input_tensor)\n",
    "        output = pad_packed_sequence(output)[0]\n",
    "        output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]         \n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder consists of LSTM and attention mechanism. It is initialized using the hidden states of the encoder and uses the vector representation from the encoder to predict the next character, conditioned on the previous predictions. For the attention mechanism, we will use hybrid + location-aware attention, explained in more detail [here](https://proceedings.neurips.cc/paper/2015/file/1068c6e4c8051cfd4e9ea8072e3189e2-Paper.pdf). The decoder is defined in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, encoder_hidden_size, attention_hidden_size, output_size, num_layers, encoder_num_layers, num_filters, batch_size, device):        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.encoder_hidden_size = encoder_hidden_size\n",
    "        self.attention_hidden_size = attention_hidden_size\n",
    "        self.num_filters = num_filters\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder_num_layers = encoder_num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.device = device\n",
    "        \n",
    "        # the embedding transforms the characters to vector representations\n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "         \n",
    "        self.lstm = nn.LSTM(self.embedding_dim,\n",
    "                            self.encoder_hidden_size,\n",
    "                            num_layers=self.num_layers,\n",
    "                            bidirectional=False)\n",
    "        self.out = nn.Linear(self.encoder_hidden_size*2, self.output_size)\n",
    "        \n",
    "        \n",
    "        # initialization of the parameters needed for the attention calculation\n",
    "        self.v = nn.Parameter(torch.FloatTensor(1, self.encoder_hidden_size).uniform_(-0.1, 0.1))\n",
    "        self.b = nn.Parameter(torch.FloatTensor(self.encoder_hidden_size).uniform_(-0.1, 0.1))\n",
    "        self.W_1 = torch.nn.Linear(self.encoder_hidden_size, self.attention_hidden_size, bias=False)\n",
    "        self.W_2 = torch.nn.Linear(self.encoder_hidden_size, self.attention_hidden_size, bias=False)\n",
    "        self.W_3 = nn.Linear(self.num_filters, self.attention_hidden_size, bias=False)\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=self.num_filters, kernel_size=3, padding=1)\n",
    "        \n",
    "    def forward(self, input_tensor, decoder_hidden, encoder_output, attn_weights):\n",
    "        embedding = self.embedding(input_tensor)\n",
    "        embedding = embedding.permute(1, 0, 2)\n",
    " \n",
    "        # we pass through the LSTM the embedding of the character and initialize the LSTM with the hidden state of the encoder\n",
    "        decoder_output, decoder_hidden = self.lstm(embedding, decoder_hidden)\n",
    " \n",
    " \n",
    "         # --------- calculation of the attention ---------\n",
    "        try:\n",
    "            conv_feat = self.conv(attn_weights).permute(0, 2, 1)\n",
    "        except:\n",
    "            random_tensor = torch.rand(encoder_output.size(1), 1, encoder_output.size(0)).to(self.device)\n",
    "            conv_feat = self.conv(F.softmax(random_tensor, dim=-1)).to(self.device).permute(0, 2, 1)\n",
    " \n",
    "        conv_feat = conv_feat.permute(1, 0, 2)\n",
    "        scores = self.hybrid_attention_score(encoder_output, decoder_output, conv_feat)\n",
    "        scores = scores.permute(1, 0, 2)\n",
    "        attn_weights = F.softmax(scores, dim=0)\n",
    " \n",
    "        context = torch.bmm(attn_weights.permute(1, 2, 0), encoder_output.permute(1, 0, 2))\n",
    "        context = context.permute(1, 0, 2)\n",
    "        output = torch.cat((context, decoder_output), -1)\n",
    "        # --------- end of the attention calculation ---------\n",
    " \n",
    " \n",
    "        output = self.out(output[0])\n",
    "        output = self.dropout(output)\n",
    "        output = F.log_softmax(output, 1)\n",
    " \n",
    "        return output, decoder_hidden, attn_weights\n",
    "    \n",
    "    \n",
    "    def hybrid_attention_score(self, encoder_output, decoder_output, conv_feat):\n",
    "        out = torch.tanh(self.W_1(decoder_output) + self.W_2(encoder_output) + self.W_3(conv_feat) + self.b)\n",
    "        v = self.v.repeat(encoder_output.data.shape[1], 1).unsqueeze(1)\n",
    "        out = out.permute(1, 0, 2)\n",
    "        v = v.permute(0, 2, 1)\n",
    "        scores = out.bmm(v)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we define the hyperparameters of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layers = 5\n",
    "decoder_layers = 1\n",
    "\n",
    "encoder_hidden_size = 150\n",
    "attention_hidden_size = 150\n",
    "\n",
    "embedding_dim_chars = 100\n",
    "num_filters = 100\n",
    "\n",
    "encoder_lr = 0.0005\n",
    "decoder_lr = 0.0005\n",
    "\n",
    "num_epochs = 10\n",
    "MAX_LENGTH = 800\n",
    "skip_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to initialize the encoder, decoder and the optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Encoder\n",
    "encoder = Encoder(features_train[0].size(1), encoder_hidden_size, encoder_layers).to(device)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=encoder_lr)\n",
    "\n",
    "# initialize the Decoder\n",
    "decoder = Decoder(embedding_dim_chars, encoder_hidden_size, attention_hidden_size, len(char2idx)+1, decoder_layers, encoder_layers, num_filters, batch_size, device).to(device)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=decoder_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, count the number of trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lstm): LSTM(40, 150, num_layers=5, bidirectional=True)\n",
      ")\n",
      "Decoder(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (embedding): Embedding(31, 100)\n",
      "  (lstm): LSTM(100, 150)\n",
      "  (out): Linear(in_features=300, out_features=31, bias=True)\n",
      "  (W_1): Linear(in_features=150, out_features=150, bias=False)\n",
      "  (W_2): Linear(in_features=150, out_features=150, bias=False)\n",
      "  (W_3): Linear(in_features=100, out_features=150, bias=False)\n",
      "  (conv): Conv1d(1, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      ")\n",
      "The number of trainable parameters is: 2624331\n"
     ]
    }
   ],
   "source": [
    "print(encoder)\n",
    "print(decoder)\n",
    "\n",
    "total_trainable_params_encoder = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "total_trainable_params_decoder = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print('The number of trainable parameters is: %d' % (total_trainable_params_encoder + total_trainable_params_decoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training**\n",
    "\n",
    "This section implements the trainng of the E2E model. As a loss function, we are going to use negative log-likelihood. The function `train()` does the training. Since training an E2E model requires a lot of time and computational power, we will skip the training and load a pre-trained model instead.\n",
    "\n",
    "Although it is not necessary for this exercise, if you want to see how the training is done, you can set the variable `skip_training` to `False`. For testing purposes, the training and development sets are the same as the test set. In practice we need to have separate training and development sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip_training == False:\n",
    "    # The criterion is the loss function that we are going to use. In this case it is the negative log-likelihood loss.\n",
    "    criterion = nn.NLLLoss(ignore_index=0, reduction='mean')\n",
    "    train(pairs_batch_train, pairs_batch_dev, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, batch_size, num_epochs, device)\n",
    "else:\n",
    "    # load the pre-trained model\n",
    "    checkpoint = torch.load('weights/state_dict_10.pt', map_location=torch.device('cpu'))\n",
    "    encoder.load_state_dict(checkpoint['encoder'])\n",
    "    decoder.load_state_dict(checkpoint['decoder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Inference**\n",
    "Next, we are going to test the model's performance. The function `greedy_decoding()` uses the trained model to generate transcripts based on audio features. The greedy decoding takes the output of the decoder, which is a probability distribution over all the characters, and picks the most probable one at each timestep. The prediction of the current character is conditioned on the previous predictions. You can familiarize yourself more with various types of decoding strategies [here](https://medium.com/voice-tech-podcast/visualising-beam-search-and-other-decoding-algorithms-for-natural-language-generation-fbba7cba2c5b#:~:text=In%20the%20greedy%20decoder%2C%20we,to%20keep%20at%20every%20step.).\n",
    "\n",
    "For assessing the performance, we are going to use the WER metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(encoder, decoder, batch_size, idx2char, test_data, MAX_LENGTH, print_predictions):\n",
    "    print('Evaluating...')\n",
    "\n",
    "    # set the encoder and the decoder to evaluation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for l, batch in enumerate(test_data):\n",
    "            pad_input_seqs, input_seq_lengths, pad_target_seqs, pad_target_seqs_lengths = batch\n",
    "            pad_input_seqs, pad_target_seqs = pad_input_seqs.to(device), pad_target_seqs.to(device)\n",
    "            \n",
    "            # pass the data through the encoder\n",
    "            encoder_output, encoder_hidden = encoder(pad_input_seqs, input_seq_lengths)\n",
    "\n",
    "            decoder_input = torch.ones(batch_size, 1).long().to(device)\n",
    "            decoder_hidden = (encoder_hidden[0].sum(0, keepdim=True), encoder_hidden[1].sum(0, keepdim=True))\n",
    "            \n",
    "            attn_weights = torch.nn.functional.softmax(torch.ones(encoder_output.size(1), 1, encoder_output.size(0)), dim=-1).to(device)\n",
    "\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "           \n",
    "            # decoding\n",
    "            for i in range(MAX_LENGTH):\n",
    "                attn_weights = attn_weights.squeeze()\n",
    "                attn_weights = attn_weights.unsqueeze(0)\n",
    "                attn_weights = attn_weights.unsqueeze(0)\n",
    "                decoder_output,  decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output, attn_weights)\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.detach()\n",
    "                \n",
    "                # if we get `<eos>`, stop the decoding\n",
    "                if topi.item() == 2:\n",
    "                    break\n",
    "                else:\n",
    "                    predictions.append(topi)\n",
    "            \n",
    "            true_labels = pad_target_seqs\n",
    "            \n",
    "            predictions = [idx2char[c.item()] for c in predictions if c.item() not in (1, 2)]\n",
    "            true_labels = [idx2char[c.item()] for c in true_labels if c.item() not in (1, 2)]\n",
    "            \n",
    "            predictions = ''.join(predictions)\n",
    "            true_labels = ''.join(true_labels)\n",
    "            \n",
    "            if print_predictions == True:\n",
    "                print('\\n')\n",
    "                print('True: ', true_labels)\n",
    "                print('Pred: ', predictions)\n",
    "\n",
    "            all_predictions.append(predictions)\n",
    "            all_labels.append(true_labels)\n",
    "\n",
    "            \n",
    "        print('\\n')\n",
    "        print('Word error rate: ', wer(all_labels, all_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "pairs_batch_test_clean = DataLoader(dataset=test_clean_data,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    collate_fn=prepare_data.collate,\n",
    "                    pin_memory=True)\n",
    "\n",
    "pairs_batch_test_other = DataLoader(dataset=test_other_data,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    collate_fn=prepare_data.collate,\n",
    "                    pin_memory=True)\n",
    "\n",
    "pairs_batch_test_long = DataLoader(dataset=test_long_data,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    collate_fn=prepare_data.collate,\n",
    "                    pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables `pairs_batch_test_clean`, `pairs_batch_test_other` and `pairs_batch_test_long` contain subsets of the LibriSpeech test set.\n",
    "With the command below, we can test the performance of the model on the data stored in `pairs_batch_test_clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "\n",
      "\n",
      "Word error rate:  0.23870417732310314\n"
     ]
    }
   ],
   "source": [
    "print_predictions = False\n",
    "greedy_decoding(encoder, decoder, batch_size, idx2char, pairs_batch_test_clean, MAX_LENGTH, print_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the cell below, we can compare the predictions against the true labels for the first 10 samples of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "\n",
      "\n",
      "True:  he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce \n",
      "Pred:  he hoped there would be stoover dinner turnips and carriets and bruised potatoes and fat mutton pieces to be lateled out and pieces to be lateled out and thick peppered flower fatten sauce \n",
      "\n",
      "\n",
      "True:  stuff it into you his belly counselled him \n",
      "Pred:  stuffered into you his belly councled him \n",
      "\n",
      "\n",
      "True:  after early nightfall the yellow lamps would light up here and there the squalid quarter of the brothels \n",
      "Pred:  after early nightfall the yellow lamps would light up here and there the squallete quarter of the brauffles \n",
      "\n",
      "\n",
      "True:  hello bertie any good in your mind \n",
      "Pred:  hellow burty and he good in your mind \n",
      "\n",
      "\n",
      "True:  number ten fresh nelly is waiting on you good night husband \n",
      "Pred:  number then fresh nell he is waiting on you could not husband \n",
      "\n",
      "\n",
      "True:  the music came nearer and he recalled the words the words of shelley's fragment upon the moon wandering companionless pale for weariness \n",
      "Pred:  the music came nearer and he recalled the words the words of shellies fragment upon the moon wandering companionless pale for weariness \n",
      "\n",
      "\n",
      "True:  the dull light fell more faintly upon the page whereon another equation began to unfold itself slowly and to spread abroad its widening tail \n",
      "Pred:  the dull light felmor faintly upon the page whereon another requation began to one fold itself slowly and to spread abroad its widening tale \n",
      "\n",
      "\n",
      "True:  a cold lucid indifference reigned in his soul \n",
      "Pred:  a cold lucid indifference reigned in his soul \n",
      "\n",
      "\n",
      "True:  the chaos in which his ardour extinguished itself was a cold indifferent knowledge of himself \n",
      "Pred:  the chaos in which his ardor extinguished itself was a cold indifferent knowledge of himself \n",
      "\n",
      "\n",
      "True:  at most by an alms given to a beggar whose blessing he fled from he might hope wearily to win for himself some measure of actual grace \n",
      "Pred:  at most by an arms given to a beggar whose blessing he fled from he might hope wearily to wind for himself some measure of actual grace \n",
      "\n",
      "\n",
      "Word error rate:  0.22023809523809523\n"
     ]
    }
   ],
   "source": [
    "print_predictions = True\n",
    "test_clean_subset = test_clean_data[:10]\n",
    "\n",
    "pairs_batch_clean_subset = DataLoader(dataset=test_clean_subset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    collate_fn=prepare_data.collate,\n",
    "                    pin_memory=True)\n",
    "greedy_decoding(encoder, decoder, batch_size, idx2char, pairs_batch_clean_subset, MAX_LENGTH, print_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "\n",
    "1. Report the WER on the data stored in `pairs_batch_test_clean` and `pairs_batch_test_other`. Use up to two decimal points.\n",
    "\n",
    "2. Why do you think the WER is worse on `pairs_batch_test_other`? Which factors could impact the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "\n",
      "\n",
      "Word error rate:  0.23870417732310314\n"
     ]
    }
   ],
   "source": [
    "print_predictions = False\n",
    "greedy_decoding(encoder, decoder, batch_size, idx2char, pairs_batch_test_clean, MAX_LENGTH, print_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "\n",
      "\n",
      "Word error rate:  0.3730108211330363\n"
     ]
    }
   ],
   "source": [
    "print_predictions = False\n",
    "greedy_decoding(encoder, decoder, batch_size, idx2char, pairs_batch_test_other, MAX_LENGTH, print_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "\n",
    "The variable `pairs_batch_test_long` contains a subset of the clean LibriSpeech test set, where the utterances are longer (longer sentences being spoken). \n",
    "\n",
    "1. Test the performance of the model on the data stored in `pairs_batch_test_long` (use up to two decimal points). How do long utterances affect the performance of the model?\n",
    "\n",
    "2. Why do you think that is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "\n",
      "\n",
      "Word error rate:  0.34807747904018504\n"
     ]
    }
   ],
   "source": [
    "print_predictions = False\n",
    "greedy_decoding(encoder, decoder, batch_size, idx2char, pairs_batch_test_long, MAX_LENGTH, print_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "\n",
    "1. As a decoding strategy, we are using greedy decoding. What are the pros and cons of this?\n",
    "\n",
    "2. Propose a better decoding algorithm that overcomes the downsides of greedy decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "\n",
    "In the current implementation of the `Encoder`, we are using a standard BLSTM for processing the input features.\n",
    "\n",
    "1. What are the issues with using this type of `Encoder`?\n",
    "\n",
    "2. How can those issues be solved?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
